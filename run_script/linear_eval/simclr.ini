## default setting is in 'Flatten args'
# Seed for Numpy and PyTorch. Default: -1 (None)
seed = 1@int

## training loop
# Epoch to start learning from, used when resuming
epoch_start = 0@int
# Total number of epochs
epochs = 90@int

# computation config
# template source : https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html
#   default - {'accelerator':'auto'}
#   cpu - {'strategy':'ddp'}
#   gpu - {'device':3, 'accelerator':'gpu', 'strategy':'ddp', 'auto_select_gpus':False}
#   tpu - {'device':3, 'accelerator':'tpu'}
# Note : ddp not good for debug, debug mode plz apply no strategy with 1 gpu device
compute_cfg = {'devices':5, 'accelerator':'gpu', 'strategy':'ddp'}@dict  

# Backbone: resnet18, 32, 50
backbone = resnet50@str
ssl_method = simclr@str
pretrain_ckpt_path = /workspace/meta_info/results/simclr_pretrain/pretrain_file/simclr-epoch=99-lin_acc1=38.70.ckpt@str

## dataset / data_loader
# Dataset: cifar10, cifar100, imagenet, slim, stl10
dataset = imagenet@str
data_dir = /data/ImgNet/train@str
# global batch : test 820 * 5 = 4100 near 4096
batch_size = 820@int

# keep 1 view 1 crop during supervised training & evaluation phase
aug_crop_lst = [1]@list
# custmized dataset (default : False)
custom_ds_cfg = {}@dict
# Number of torchvision workers used to load data (default: 8)
num_workers = 18@int

## logger (default CSV logger, we only need to define log_path and proj_name)
log_type = wandb@str
[wandb_args]
save_dir = /workspace/meta_info@str
name = simclr@str
project = lw_ssl@str
entity = josef@str
offline = @bool 

[ckpt_args]
only_eval = @bool
#ckpt_path = /workspace/meta_info/results/simsiam_pretrain/linear_eval_file/exp1_b4100_ep90_ac070_acf88.ckpt@str
dirpath = /workspace/meta_info/ckpts@str
filename = simclr@str
every_n_epochs = 30@int
save_top_k = -1@int
monitor = lin_loss@str

[ssl_args]
proj_hidden_dim = 2048@int
proj_output_dim = 128@int